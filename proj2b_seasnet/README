NAME: Ege Tanboga
EMAIL: ege72282@gmail.com
ID: 304735411

Resources utilized: https://stackoverflow.com/questions/6626685/how-does-pthread-join-populate-the-variable-of-thread-result

Hash function taken from:

http://www.cs.yale.edu/homes/aspnes/pinewiki/C(2f)HashTables.html?highlight=%28CategoryAlgorithmNotes%29 (Seen in multiplication method)

SortedList.h: Interface for sorted list
SortedList.c: implementation of functions of sorted list
lab2_list.c: adds list partitioning to the implementation from project 2a
lab2_list.gp: provides the scripts to construct graphs using gnu plot
lab2b_list.csv: contains results for test runs
Makefile: contains commands to be run which aid in constructing the graphs and providing the profile
profile.out: execution profiling report, shows time spent in the unpartitioned spin lock implementation

*png -> graphs as specified in the spec, constructed by the script lab2_list.gp

lab2b_1.png ... throughput vs number of threads for mutex and spin-lock synchronized list operations.
lab2b_2.png ... mean time per mutex wait and mean time per operation for mutex-synchronized list operations.
lab2b_3.png ... successful iterations vs threads for each synchronization method.
lab2b_4.png ... throughput vs number of threads for mutex synchronized partitioned lists.
lab2b_5.png ... throughput vs number of threads for spin-lock-synchronized partitioned lists

QUESTION 2.3.1 - Cycles in the basic list implementation:

Where do you believe most of the cycles are spent in the 1 and 2-thread list tests ?

Why do you believe these to be the most expensive parts of the code?

Combined answer for the first two questions:

4 different cases: 
1-thread with spin lock -> most operations go to list operations as we only have one thread and there is no other thread spinning
1-thread with mutex -> intuition: if list large, most cycles in list operations. If list small, then we do not know and we need to write experiments to verify this
2-thread with spin lock, -> we have two threads, when one thread is modifying the list, the other one is spinning. Hence, we can say 50% of CPU cycles go to spinning and 50% to list operations
2-thread with mutex -> if list large -> most cycles go to list operations. If list small -> it is possible that mutex will take most of the CPU cycle

Where do you believe most of the time/cycles are being spent in the high-thread spin-lock tests?

In spin lock - while spinning

Where do you believe most of the time/cycles are being spent in the high-thread mutex tests?

It is possible that most cycles are spent in the list operations assuming that the list is large


QUESTION 2.3.2 - Execution Profiling:
Where (what lines of code) are consuming most of the cycles when the spin-lock version of the list exerciser is run with a large number of threads?

Why does this operation become so expensive with large numbers of threads?

The most time is spent in the spin lock implementation, the function manipulate_list_spin.  The while condition where the waiting thread spin obtains the most CPU cycles in that function. This gets more expensive with increasing threads as there is high contention for the spin lock which means many threads are spinning waiting for the lock to be unlocked. So if there are n threads, then one of them will have the lock and n-1 of them will be spinning.

QUESTION 2.3.3 - Mutex Wait Time:
Look at the average time per operation (vs # threads) and the average wait-for-mutex time (vs #threads).
Why does the average lock-wait time rise so dramatically with the number of contending threads?
Why does the completion time per operation rise (less dramatically) with the number of contending threads?
How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?

The average lock wait time increases dramatically with increasing contending threads as there are more threads in the queue, waiting to acquire the lock. With many threads waiting for the same one, there will be many threads sleeping and more collective time spent waiting. 

Completion time per operation rises less dramatically because of caching. If we have a lot of threads, then when a thread has the lock L1 and L2 will contain useless data and this will inhibit us from using caching effectively, and resort to memory, which increases the completion time per operation

The time to complete a list operation is roughly constant (if the list is of a certain size). However, as number of threads increase, contention will increase which means threads will spend more time waiting than performing the operation. Hence, the wait time per operation goes up faster

QUESTION 2.3.4 - Performance of Partitioned Lists
Explain the change in performance of the synchronized methods as a function of the number of lists.

As we add more lists, throughput increases as there is less contention for a lock on a single list and the lists are smaller

Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.

If we continue increasing the number of sublists, the list size will keep getting smaller. Insert and lookup are both O(n). The time for each operation is going to keep decreasing, which means we will spend fewer time in the list operation. Eventually we will reach an upper bound for throughput (when we reach the number of cores)

It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. Does this appear to be true in the above curves? If not, explain why not.

##ANSWER AFTER LOOKING AT GRAPHS


